 print "Hello World"

.create table logsRaw(Timestamp:datetime, Source:string, Node:string, Level:string, Component:string, ClientRequestId:string, Message:string, Properties:dynamic) 

  logsRaw
  | count 

  logsRaw
  | take 10

  select count() from logsRaw

explain select max(Timestamp) as MaxTimestamp 
from logsRaw 
where Level='Error'

// Challenge 3, Task 1: Basic KQL queries - explore the data

logsRaw
| where Level=="Error"
| take 10

logsRaw
| summarize count() // or: count

logsRaw
| summarize min(Timestamp), max(Timestamp)

logsRaw
| where Component == "DOWNLOADER"
| take 100
| extend originalSize=Properties.OriginalSize, compressedSize=Properties.compressedSize

// Challenge 3, Task 2: Explore the table and columns 🎓
logsRaw
| where Component == "DOWNLOADER"
| take 100
| extend originalSize=tolong(Properties.OriginalSize), compressedSize=tolong(Properties.compressedSize)
//| getschema

// Challenge 3, Task 3: Keep the columns of your interest 🎓
logsRaw
| project Timestamp, ClientRequestId, Level, Message
| take 10


//Challenge 3, Task 4: Filter the output 🎓
logsRaw
| where Timestamp between (datetime(2014-03-08 01:00) .. datetime(2014-03-08 10:00))
| project Timestamp, ClientRequestId, Level, Message
| take 10 

//Challenge 3, Task 5: Sorting the results 🎓
logsRaw
| where Component == "INGESTOR_EXECUTER"
| extend rowCount=tolong(Properties.rowCount)
//| sort by rowCount asc
| top 10 by rowCount desc


// Challenge 3, Task 6: Reorder, rename, add columns 🎓
logsRaw
| where Component == "INGESTOR_EXECUTER"
| extend fileFormat=(Properties.format), rowCount=tolong(Properties.rowCount)
//| project-rename fileFormat=format
| project-reorder Timestamp, fileFormat, rowCount * asc

// Challenge 3, Task 7: Total number of records 🎓
logsRaw
| summarize Count=count() by Component

// Challenge 3, Task 8: Aggregations and string operations 🎓
logsRaw
| where Message contains "ingestion"
| summarize count() by Level

// Challenge 3, Task 9: Render a chart 🎓
logsRaw
| summarize count() by Level
| render piechart 

// Challenge 3, Task 10: Create bins and visualize time series 🎓
logsRaw
| summarize count() by bin(Timestamp, 30m)
| render timechart


logsRaw
| where Component in ('INGESTOR_EXECUTER', 'INGESTOR_GATEWAY', 'INTEGRATIONDATABASE','INTEGRATIONSERVICEFLOWS', 'INTEGRATIONSERVICETRACE', 'DOWNLOADER')

// Challenge 4, Task 1: User defined Function (Stored Functions) 🎓

.create table ingestionLogs (Timestamp: datetime, Source: string,Node: string, Level: string, Component: string, ClientRequestId: string, Message: string, Properties: dynamic)

// Create a function for the update policy 🎓
.create function ManipulatelogsRaw() { 
logsRaw
| where Component in ('INGESTOR_EXECUTER', 'INGESTOR_GATEWAY', 'INTEGRATIONDATABASE','INTEGRATIONSERVICEFLOWS', 'INTEGRATIONSERVICETRACE', 'DOWNLOADER')
}

ManipulatelogsRaw
| take 10

// Challenge 4, Task 2: Create an update policy 🎓
.alter table ingestionLogs policy update 
@'[{ "IsEnabled": true, "Source": "logsRaw", "Query": "ManipulatelogsRaw()"}]'

.set-or-append logsRaw <| logsRaw | take 100000

ingestionLogs
| count

logsRaw
| count

// Challenge 5, Task 1: Change the retention policy via commands 🎓
.alter table ingestionLogs policy retention
```
{
    "SoftDeletePeriod": "180.00:00:00",
    "Recoverability": "Enabled"
}
```
.show table ingestionLogs policy retention 

// Challenge 6, Task 2: .show/diagnostic logs/Insights
.show queries
| where StartedOn > ago(3h)
| count
 
// Challenge 6, Task 3: Use .journal commands 
.show journal
| where Event == "CREATE-TABLE"
| project EventTimestamp

//Challenge 6, Task 4: Use .show commands 
.show commands
| where StartedOn > ago(4h)
| summarize count() by User

// Challenge 6, Task 5: Table details and size
.show table ingestionLogs details
| extend formatTotalExtentSize = format_bytes(TotalExtentSize, 2, "MB"), formatOriginalSize = format_bytes(TotalOriginalSize, 2, "MB")
| project formatTotalExtentSize, formatOriginalSize


// Challenge 7, Task 1: Declaring variables and using 'let' statements 🎓

// let LogType='Warning';
// let TimeBucket=1m;
// ingestionLogs
// | where Level==LogType
// | summarize count() by bin(Timestamp,TimeBucket)

let NodesOfTop3Files = 'Warning';
logsRaw
| where Component == "INGESTOR_EXECUTER"
| extend fileSize=tolong(Properties.size)
| top 3 by NodesOfTop3Files 
| project Message;
logsRaw 
| where Node in (NodesOfTop3Files)
| where Message startswith "Exception"

// Challenge 7, Task 2: Use the search operator 🎓

logsRaw
| search "Exception=System.Timeout"

// Challenge 7, Task 3: Parse Key-Value pairs strings into separate columns 🎓
logsRaw
| where Component == "INGESTOR_GATEWAY"
| parse-kv Message as (table:string, format:string) with (pair_delimiter=' ', kv_delimiter='=', greedy=true)
| summarize count() by format

// Challenge 7, Task 4: Nulls are important in timeseries analysis (Compare summarize and make-series)

//Part 1: 
logsRaw
| where Node == "Engine000000000378" and Component == "INGESTOR_EXECUTER"
| extend fileSize=tolong(Properties.size)
| summarize count() by bin(Timestamp, 30m)
| render timechart

//Part 2: 

let Timebuckets = 30m;
logsRaw
| where Node == "Engine000000000378" and Component == "INGESTOR_EXECUTER"
| extend fileSize=tolong(['Properties']['size'])
| make-series series=avg(fileSize) on Timestamp step Timebuckets 
| render timechart

// Challenge 7, Task 5: Anomaly detection 🎓

let Timebuckets = 30m;
logsRaw 
| where Component == "INGESTOR_EXECUTER"
| extend fileSize=tolong(Properties.size)
| make-series records_series=sum(fileSize) on Timestamp step Timebuckets 
| extend ActualUsage = series_decompose_anomalies(records_series, 1, -1, 'linefit')
| render anomalychart with(anomalycolumns=ActualUsage, title="file size anomalies")


logsRaw 
| where Component == "INGESTOR_EXECUTER"
| extend fileSize=tolong(Properties.size)
| make-series ActualSize=sum(fileSize) on Timestamp step 5min // Creates the time series, listed by data type
| extend(AnomalyFlags, AnomalyScore, PredictedSize) = series_decompose_anomalies(ActualSize, -1) // Scores and extracts anomalies based on the output of make-series 
| mv-expand ActualSize to typeof(double), Timestamp to typeof(datetime), AnomalyFlags to typeof(double),AnomalyScore to typeof(double), PredictedSize to typeof(long) // Expands the array created by series_decompose_anomalies()
| where AnomalyFlags != 0  // Returns all positive and negative deviations from expected usage
| project Timestamp,ActualSize = format_bytes(ActualSize, 2),PredictedSize = format_bytes(PredictedSize, 2), AnomalyScore, AnomalyFlags // Defines which columns to return 
| sort by abs(AnomalyScore) desc // Sorts results by anomaly score in descending ordering

// Challenge 8: Visualization

// Challenge 8, Task 1: Prepare interactive dashboards with ADX Dashboard 🎓

// Query 1 
ingestionLogs
| where Timestamp between (todatetime(_startTime) .. todatetime(_endTime))
| summarize count() by bin(Timestamp, 10m), Component
| render timechart 

// Query 2 
let TimeBuckets = 10m;
ingestionLogs 
| where Timestamp between (datetime(2014-03-08T07:00:00) .. datetime(2014-03-08T12:00:00))
| make-series MySeries=count() on Timestamp step TimeBuckets by Level
| extend anomaly = series_decompose_anomalies(MySeries)
| render anomalychart 

// Query 3 
ingestionLogs
| where Timestamp between(todatetime(_startTime)..todatetime(_endTime))
| summarize count() by Level
| render piechart 